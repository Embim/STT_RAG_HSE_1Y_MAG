# RAG Pipeline - Сводка по проекту

## Описание проекта

RAG (Retrieval-Augmented Generation) пайплайн для обработки и поиска по транскрибированным лекциям. Система позволяет:

1. Загружать текстовые данные из JSON файлов
2. Разбивать текст на семантические чанки (фрагменты)
3. Векторизовать текст с помощью современной модели эмбеддингов
4. Индексировать векторы в Weaviate (векторная БД)
5. Выполнять семантический поиск по базе знаний

## Технологический стек

### Python библиотеки
- **sentence-transformers** - векторизация текста
- **weaviate-client** - работа с векторной БД
- **transformers** - трансформерные модели
- **tqdm** - прогресс-бары
- **numpy** - численные вычисления
- **scikit-learn** - ML утилиты

### Модель эмбеддингов
- **Qwen/Qwen3-Embedding-0.6B** (768-мерные векторы)
- Поддержка русского языка
- Специализирована для поиска

### Инфраструктура
- **Weaviate** - векторная база данных
- **Docker** - контейнеризация Weaviate
- **Python 3.8+**

## Архитектура

```
┌──────────────────┐
│  JSON Лекции     │
│  (40 файлов)     │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ LectureProcessor │  ← Загрузка
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  TextChunker     │  ← Разбиение на чанки (500 слов, overlap 50)
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ EmbeddingModel   │  ← Векторизация (Qwen3-Embedding)
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ WeaviateIndexer  │  ← Индексация в БД
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  RAGRetriever    │  ← Поиск по запросу
└──────────────────┘
```

## Компоненты системы

### 1. Основной пайплайн (rag_pipeline.py)
Содержит все основные классы:
- `LectureProcessor` - загрузка JSON
- `TextChunker` - разбиение на чанки
- `EmbeddingModel` - векторизация
- `WeaviateIndexer` - индексация
- `RAGRetriever` - поиск

### 2. Интерактивный поиск (interactive_search.py)
CLI интерфейс для поиска по базе знаний в реальном времени.

### 3. Статистика (stats.py)
Анализ проиндексированных данных:
- Количество чанков
- Распределение по лекциям
- Размеры чанков

### 4. Примеры (examples.py)
6 готовых примеров использования API:
- Простой поиск
- Обработка лекций
- Кастомный chunking
- Batch поиск
- Анализ эмбеддингов
- Фильтрация по лекциям

### 5. Утилиты
- `quickstart.bat/sh` - быстрый старт
- `docker-compose.yml` - конфигурация Weaviate
- `requirements.txt` - зависимости

## Workflow использования

### Первый запуск

1. **Запустить Weaviate**
   ```bash
   docker-compose up -d
   ```

2. **Установить зависимости**
   ```bash
   pip install -r requirements.txt
   ```

3. **Проиндексировать лекции**
   ```bash
   python rag_pipeline.py
   ```

### Регулярное использование

**Поиск:**
```bash
python interactive_search.py
```

**Статистика:**
```bash
python stats.py
```

**Примеры в коде:**
```python
from rag_pipeline import RAGRetriever, EmbeddingModel, WeaviateIndexer

model = EmbeddingModel()
indexer = WeaviateIndexer()
indexer.connect()

retriever = RAGRetriever(indexer, model)
results = retriever.retrieve("машинное обучение", top_k=5)
```

## Параметры и настройки

### Chunking параметры
- `CHUNK_SIZE` = 500 слов (настраиваемо)
- `OVERLAP` = 50 слов (настраиваемо)

### Векторизация
- Batch size = 32 (настраиваемо)
- Размерность = 768 (фиксировано моделью)

### Weaviate
- URL = http://localhost:8080 (настраиваемо)
- Collection = "LectureChunks"

## Производительность

### Тестовые данные
- 40 лекций
- ~2,000,000 слов общего текста
- ~4,000 чанков после разбиения

### Время обработки (примерное)
- Загрузка: ~5 сек
- Chunking: ~10 сек
- Векторизация: ~5-10 мин (CPU) / ~1-2 мин (GPU)
- Индексация: ~30 сек
- Поиск: <1 сек

### Требования к ресурсам
- RAM: минимум 8 GB (рекомендуется 16 GB)
- Disk: ~5 GB (модель + данные + Docker)
- GPU: опционально (значительно ускоряет векторизацию)

## Возможности расширения

### 1. Улучшение chunking
- Semantic chunking (разбиение по смыслу)
- Разбиение по предложениям (NLTK, spaCy)
- Адаптивный размер чанков

### 2. Улучшение поиска
- Гибридный поиск (векторный + BM25)
- Re-ranking результатов
- Учет метаданных (дата, автор, тема)

### 3. Интеграция с LLM
- Генерация ответов на основе найденных чанков
- Summarization найденной информации
- Question answering

### 4. UI/UX
- Web интерфейс (Streamlit, Gradio)
- REST API (FastAPI)
- Telegram/Slack бот

### 5. Мониторинг
- Логирование запросов
- Метрики качества поиска
- A/B тестирование

## Ограничения и известные проблемы

### Текущие ограничения
1. Только текстовые данные (нет поддержки изображений из презентаций)
2. Нет учета временных меток из segments
3. Простая стратегия chunking (fixed window)
4. Нет кеширования эмбеддингов

### Планы по улучшению
1. Добавить поддержку сегментов с временными метками
2. Реализовать semantic chunking
3. Добавить кеширование для ускорения
4. Интегрировать с LLM для генерации ответов

## Тестирование

### Ручное тестирование
```bash
python interactive_search.py
```

Примеры запросов:
- "Что такое машинное обучение?"
- "Какие бывают типы признаков?"
- "Объясни нейронные сети"

### Программное тестирование
```bash
python examples.py
```

## Документация

- [README.md](README.md) - основная документация
- [QUICKREF.md](QUICKREF.md) - быстрая справка
- [PROJECT_SUMMARY.md](PROJECT_SUMMARY.md) - этот файл

## Лицензия

MIT License - свободное использование в образовательных и коммерческих целях.

## Контакты и поддержка

При возникновении проблем:
1. Проверьте [README.md](README.md) раздел "Troubleshooting"
2. Проверьте [QUICKREF.md](QUICKREF.md) для быстрых решений
3. Изучите [examples.py](examples.py) для понимания API

## Дата создания

Создано: 2025-10-19
Версия: 1.0.0
